{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnoushkaKareCode111/AnoushkaKareCode111/blob/main/IMU_LSTM_pipeline_zenodo_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83fc70a2",
      "metadata": {
        "id": "83fc70a2"
      },
      "source": [
        "# IMU-based HAR & Payload Estimation\n",
        "This notebook reproduces the experiments from the paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02fbdf29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02fbdf29",
        "outputId": "ebd39755-1427-4c0b-d5fb-13305b385e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q scikit-learn scipy nbformat\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "\n",
        "import os, glob, re, math, random, json, time\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from scipy.signal import butter, filtfilt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "seed = 1234\n",
        "np.random.seed(seed); random.seed(seed); tf.random.set_seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Reproducibility seeds ====\n",
        "import os, random, numpy as np, tensorflow as tf\n",
        "\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Random seed fixed to\", SEED)\n",
        "# ===============================\n"
      ],
      "metadata": {
        "id": "q3Z8znUSk7oV",
        "outputId": "04906915-4dc8-435b-eac9-5278ebfd0d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q3Z8znUSk7oV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed fixed to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ced8b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "91ced8b9",
        "outputId": "4cba7026-13d1-4b4e-a347-28325aa94c51"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3237532956.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# EDIT: set this to the folder where you put the Zenodo CSV files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mBASE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/imu_project/\"\u001b[0m  \u001b[0;31m# <<-- change if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# EDIT: set this to the folder where you put the Zenodo CSV files\n",
        "BASE_PATH = \"/content/drive/MyDrive/imu_project/\"  # <<-- change if needed\n",
        "\n",
        "files = os.listdir(BASE_PATH)\n",
        "subjects = sorted({ fname.split('_')[0] for fname in files if fname.startswith('U') })\n",
        "print('Detected subjects (files present):', subjects)\n",
        "expected = [f\"U{str(i).zfill(3)}\" for i in range(1,13)]\n",
        "print('Expected subjects:', expected)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b961d36",
      "metadata": {
        "id": "6b961d36"
      },
      "outputs": [],
      "source": [
        "def detect_label_columns(df):\n",
        "    cols = df.columns.str.lower()\n",
        "    label_candidates = [c for c in df.columns if re.search(r'(label|action|intent|intention|class)', c, re.I)]\n",
        "    payload_candidates = [c for c in df.columns if re.search(r'(payload|weight|mass)', c, re.I)]\n",
        "    timestamp_candidates = [c for c in df.columns if re.search(r'(time|timestamp|ts)', c, re.I)]\n",
        "    return {\n",
        "        'label_col': label_candidates[0] if label_candidates else None,\n",
        "        'payload_col': payload_candidates[0] if payload_candidates else None,\n",
        "        'timestamp_col': timestamp_candidates[0] if timestamp_candidates else None\n",
        "    }\n",
        "\n",
        "def detect_sensor_columns(df, exclude_cols):\n",
        "    sensor_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in exclude_cols]\n",
        "    return sensor_cols\n",
        "\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "def lowpass_filtfilt(df, cols, fs=100, cutoff=5.0, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    out = df.copy()\n",
        "    for c in cols:\n",
        "        try:\n",
        "            out[c] = filtfilt(b, a, df[c].astype(float).to_numpy(), padlen=3*(max(len(a),len(b))-1))\n",
        "        except Exception:\n",
        "            out[c] = df[c].astype(float).to_numpy()\n",
        "    return out\n",
        "\n",
        "\n",
        "def sliding_windows(X, y, window_size, step):\n",
        "    Xs, ys = [], []\n",
        "    T = X.shape[0]\n",
        "    for start in range(0, T - window_size + 1, step):\n",
        "        end = start + window_size\n",
        "        Xw = X[start:end, :]\n",
        "        yw = y[start:end]\n",
        "        vals, counts = np.unique(yw, return_counts=True)\n",
        "        label = vals[np.argmax(counts)]\n",
        "        Xs.append(Xw)\n",
        "        ys.append(label)\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "\n",
        "def derive_action_and_interaction(labels):\n",
        "    actions = []\n",
        "    inters  = []\n",
        "    for v in labels:\n",
        "        s = str(v).lower()\n",
        "        if 'walk' in s:\n",
        "            actions.append('walk'); inters.append('none')\n",
        "        elif 'stand' in s or 'idle' in s:\n",
        "            actions.append('stand'); inters.append('none')\n",
        "        elif 'lift' in s:\n",
        "            actions.append('interact'); inters.append('lift')\n",
        "        elif 'lower' in s or 'put' in s:\n",
        "            actions.append('interact'); inters.append('lower')\n",
        "        else:\n",
        "            actions.append('stand'); inters.append('none')\n",
        "    return np.array(actions), np.array(inters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d69204",
      "metadata": {
        "id": "c4d69204"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_har_model(input_shape, n_action=3, n_inter=2):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LSTM(100, return_sequences=True)(inputs)\n",
        "    x = layers.LSTM(50)(x)\n",
        "    x = layers.Dense(20, activation='relu')(x)\n",
        "    out_action = layers.Dense(n_action, activation='softmax', name='action')(x)\n",
        "    out_inter  = layers.Dense(n_inter, activation='softmax', name='interaction')(x)\n",
        "    model = models.Model(inputs, [out_action, out_inter])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss={'action':'categorical_crossentropy', 'interaction':'categorical_crossentropy'},\n",
        "                  metrics={'action':'accuracy','interaction':'accuracy'})\n",
        "    return model\n",
        "\n",
        "def build_payload_model(input_shape, n_classes=3):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LSTM(100, return_sequences=True)(inputs)\n",
        "    x = layers.LSTM(50)(x)\n",
        "    x = layers.Dense(20, activation='relu')(x)\n",
        "    out_payload = layers.Dense(n_classes, activation='softmax', name='payload')(x)\n",
        "    model = models.Model(inputs, out_payload)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a99c1c7",
      "metadata": {
        "id": "6a99c1c7"
      },
      "outputs": [],
      "source": [
        "FS = 100\n",
        "CUTOFF = 5.0\n",
        "WIN_SEC = 1.0\n",
        "WIN = int(WIN_SEC * FS)\n",
        "STEP = WIN // 2\n",
        "\n",
        "all_results = []\n",
        "\n",
        "FAST_MODE = True   # change to False to run ALL 12 subjects\n",
        "\n",
        "if FAST_MODE:\n",
        "    subj_list = [\"U001\"]   # runs quickly\n",
        "else:\n",
        "    subj_list = [f\"U{str(i).zfill(3)}\" for i in range(1,13)]  # full experiment\n",
        "\n",
        "\n",
        "for SUBJ in subj_list:\n",
        "    print('\\n===== SUBJECT', SUBJ, '=====')\n",
        "    patt = os.path.join(BASE_PATH, f\"{SUBJ}_*.csv\")\n",
        "    files = sorted(glob.glob(patt))\n",
        "    if not files:\n",
        "        print('No files for', SUBJ, '- skipping')\n",
        "        continue\n",
        "    data = { os.path.basename(f): pd.read_csv(f, low_memory=False) for f in files }\n",
        "    train_int = data.get(f\"{SUBJ}_train_intention.csv\")\n",
        "    val_int   = data.get(f\"{SUBJ}_val_intention.csv\")\n",
        "    test_int  = data.get(f\"{SUBJ}_test_intention.csv\")\n",
        "    if train_int is None or test_int is None:\n",
        "        print('Missing intention files for', SUBJ); continue\n",
        "    train_int_full = pd.concat([train_int, val_int], ignore_index=True) if val_int is not None else train_int\n",
        "    det = detect_label_columns(train_int_full)\n",
        "    LABEL_COL_INTENTION = det['label_col'] or det['payload_col'] or train_int_full.columns[-1]\n",
        "    TIMESTAMP_COL = det['timestamp_col']\n",
        "    exclude = [LABEL_COL_INTENTION] + ([TIMESTAMP_COL] if TIMESTAMP_COL else [])\n",
        "    sensor_cols = detect_sensor_columns(train_int_full, exclude)\n",
        "    train_int_filt = lowpass_filtfilt(train_int_full, sensor_cols, fs=FS, cutoff=CUTOFF, order=4)\n",
        "    test_int_filt  = lowpass_filtfilt(test_int, sensor_cols, fs=FS, cutoff=CUTOFF, order=4)\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    scaler.fit(train_int_filt[sensor_cols])\n",
        "    Xtr = scaler.transform(train_int_filt[sensor_cols])\n",
        "    Xte = scaler.transform(test_int_filt[sensor_cols])\n",
        "    ytr_raw = train_int_filt[LABEL_COL_INTENTION].values\n",
        "    yte_raw = test_int_filt[LABEL_COL_INTENTION].values\n",
        "    Xtr_seq, ytr_win = sliding_windows(Xtr, ytr_raw, WIN, STEP)\n",
        "    Xte_seq, yte_win = sliding_windows(Xte, yte_raw, WIN, STEP)\n",
        "    if len(Xtr_seq)==0 or len(Xte_seq)==0:\n",
        "        print('Not enough windows for', SUBJ); continue\n",
        "    action_tr, inter_tr = derive_action_and_interaction(ytr_win)\n",
        "    action_te, inter_te = derive_action_and_interaction(yte_win)\n",
        "    le_act = LabelEncoder(); le_act.fit(np.concatenate([action_tr, action_te]))\n",
        "    le_int = LabelEncoder(); le_int.fit(np.concatenate([inter_tr, inter_te]))\n",
        "    Yact_tr = to_categorical(le_act.transform(action_tr))\n",
        "    Yact_te = to_categorical(le_act.transform(action_te))\n",
        "    Yint_tr = to_categorical(le_int.transform(inter_tr))\n",
        "    Yint_te = to_categorical(le_int.transform(inter_te))\n",
        "    input_shape = Xtr_seq.shape[1:]\n",
        "    har_model = build_har_model(input_shape, n_action=Yact_tr.shape[1], n_inter=Yint_tr.shape[1])\n",
        "    es = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "    history = har_model.fit(Xtr_seq, {'action':Yact_tr, 'interaction':Yint_tr},\n",
        "                            validation_data=(Xte_seq, {'action':Yact_te, 'interaction':Yint_te}),\n",
        "                            epochs=60, batch_size=64, callbacks=[es], verbose=0)\n",
        "    pred_act, pred_int = har_model.predict(Xte_seq, verbose=0)\n",
        "    yhat_act = le_act.inverse_transform(np.argmax(pred_act, axis=1))\n",
        "    yhat_int = le_int.inverse_transform(np.argmax(pred_int, axis=1))\n",
        "    f1_act = f1_score(action_te, yhat_act, average='weighted')\n",
        "    f1_int = f1_score(inter_te, yhat_int, average='weighted')\n",
        "    acc_act = accuracy_score(action_te, yhat_act)\n",
        "    acc_int = accuracy_score(inter_te, yhat_int)\n",
        "    print('HAR action F1:', round(f1_act,3), 'interaction F1:', round(f1_int,3))\n",
        "    train_pay = data.get(f\"{SUBJ}_train_payload.csv\")\n",
        "    val_pay   = data.get(f\"{SUBJ}_val_payload.csv\")\n",
        "    test_pay  = data.get(f\"{SUBJ}_test_payload.csv\")\n",
        "    if train_pay is None or test_pay is None:\n",
        "        print('Missing payload files for', SUBJ); continue\n",
        "    train_pay_full = pd.concat([train_pay, val_pay], ignore_index=True) if val_pay is not None else train_pay\n",
        "    detp = detect_label_columns(train_pay_full)\n",
        "    PAY_COL = detp['payload_col'] or train_pay_full.columns[-1]\n",
        "    sensor_cols_pay = detect_sensor_columns(train_pay_full, [PAY_COL])\n",
        "    train_pay_filt = lowpass_filtfilt(train_pay_full, sensor_cols_pay, fs=FS, cutoff=CUTOFF, order=4)\n",
        "    test_pay_filt  = lowpass_filtfilt(test_pay, sensor_cols_pay, fs=FS, cutoff=CUTOFF, order=4)\n",
        "    scaler_pay = MinMaxScaler(feature_range=(-1,1))\n",
        "    scaler_pay.fit(train_pay_filt[sensor_cols_pay])\n",
        "    Xtr_pay = scaler_pay.transform(train_pay_filt[sensor_cols_pay])\n",
        "    Xte_pay = scaler_pay.transform(test_pay_filt[sensor_cols_pay])\n",
        "    ytr_pay_raw = train_pay_filt[PAY_COL].values\n",
        "    yte_pay_raw = test_pay_filt[PAY_COL].values\n",
        "    Xtr_pay_seq, ytr_pay_win = sliding_windows(Xtr_pay, ytr_pay_raw, WIN, STEP)\n",
        "    Xte_pay_seq, yte_pay_win = sliding_windows(Xte_pay, yte_pay_raw, WIN, STEP)\n",
        "    if len(Xtr_pay_seq)==0 or len(Xte_pay_seq)==0:\n",
        "        print('Not enough payload windows for', SUBJ); continue\n",
        "    le_pay = LabelEncoder(); le_pay.fit(np.concatenate([ytr_pay_win, yte_pay_win]))\n",
        "    Ytr_pay = to_categorical(le_pay.transform(ytr_pay_win))\n",
        "    Yte_pay = to_categorical(le_pay.transform(yte_pay_win))\n",
        "    payload_model = build_payload_model(Xtr_pay_seq.shape[1:], n_classes=Ytr_pay.shape[1])\n",
        "    es2 = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "    payload_model.fit(Xtr_pay_seq, Ytr_pay, validation_data=(Xte_pay_seq, Yte_pay), epochs=60, batch_size=64, callbacks=[es2], verbose=0)\n",
        "    pred_pay = payload_model.predict(Xte_pay_seq, verbose=0)\n",
        "    yhat_pay = le_pay.inverse_transform(np.argmax(pred_pay, axis=1))\n",
        "    f1_pay = f1_score(yte_pay_win, yhat_pay, average='weighted')\n",
        "    acc_pay = accuracy_score(yte_pay_win, yhat_pay)\n",
        "    print('Payload F1:', round(f1_pay,3), 'Acc:', round(acc_pay,3))\n",
        "    all_results.append({'subject':SUBJ,\n",
        "                        'har_action_f1':float(f1_act),'har_inter_f1':float(f1_int),'har_action_acc':float(acc_act),'har_inter_acc':float(acc_int),\n",
        "                        'payload_f1':float(f1_pay),'payload_acc':float(acc_pay)})\n",
        "    tf.keras.backend.clear_session()\n",
        "    time.sleep(1)\n",
        "\n",
        "# Save CSV of results and compute medians\n",
        "import pandas as pd\n",
        "if len(all_results)>0:\n",
        "    df_res = pd.DataFrame(all_results)\n",
        "    out_csv = os.path.join(BASE_PATH, 'all_subjects_results.csv')\n",
        "    df_res.to_csv(out_csv, index=False)\n",
        "    print('\\nSaved results to', out_csv)\n",
        "    print('Median HAR action F1:', df_res['har_action_f1'].median())\n",
        "    print('Median HAR interaction F1:', df_res['har_inter_f1'].median())\n",
        "    print('Median Payload F1:', df_res['payload_f1'].median())\n",
        "else:\n",
        "    print('No results to save.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ebd41d",
      "metadata": {
        "id": "c3ebd41d"
      },
      "outputs": [],
      "source": [
        "res_path = os.path.join(BASE_PATH, 'all_subjects_results.csv')\n",
        "if os.path.exists(res_path):\n",
        "    df_res = pd.read_csv(res_path)\n",
        "    display(df_res)\n",
        "    print('Medians:')\n",
        "    print('HAR action F1 median:', df_res['har_action_f1'].median())\n",
        "    print('HAR interaction F1 median:', df_res['har_inter_f1'].median())\n",
        "    print('Payload F1 median:', df_res['payload_f1'].median())\n",
        "else:\n",
        "    print('Results file not found. Run previous cell first.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a88e7b3b",
      "metadata": {
        "id": "a88e7b3b"
      },
      "source": [
        "### Notes & troubleshooting\n",
        "- If auto-detection of columns fails, set label column names manually in the code before running the loop.\n",
        "- If you run out of RAM/GPU, consider processing subjects one-by-one or reduce batch_size.\n",
        "- The notebook trains subject-specific models; to reproduce paper medians, ensure you process all U001..U012.\n",
        "- The window length is 1 second with 50% overlap; you can tune WIN_SEC variable.\n",
        "\n",
        "After you download the notebook, upload it to your Drive and open in Colab. If you want, I can now provide the download link to the generated notebook file."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}